{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ulPc7FG_RvTa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Google Drive (for Google Colab):"
      ],
      "metadata": {
        "id": "bZBdLQhpRxwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive if running in Google Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RL9dO-6rRzEM",
        "outputId": "af1b50c9-3802-42c9-a4a7-e2ef74df1aab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries:"
      ],
      "metadata": {
        "id": "67BUT8tnR5RC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input\n",
        "from tensorflow.keras.models import Model\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "from string import digits\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "ZZj7O8XbSD37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read and Preprocess Data:"
      ],
      "metadata": {
        "id": "-LOeV2wNSG9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/NLP_LAB/Hindi_English_Truncated_Corpus.csv')\n",
        "\n",
        "# Filter data for short sentences\n",
        "data = data[(data.english_sentence.apply(lambda x: len(str(x)) <= 30)) &\n",
        "            (data.hindi_sentence.apply(lambda x: len(str(x)) <= 30))]\n",
        "\n",
        "# Convert sentences to lowercase\n",
        "data['english_sentence'] = data['english_sentence'].apply(lambda x: str(x).lower())\n",
        "data['hindi_sentence'] = data['hindi_sentence'].apply(lambda x: x.lower())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Glp_pudSLUu",
        "outputId": "75d9de6b-e3cd-4fc6-8f85-de12fd42fabc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-b62a75121f97>:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['english_sentence'] = data['english_sentence'].apply(lambda x: str(x).lower())\n",
            "<ipython-input-3-b62a75121f97>:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['hindi_sentence'] = data['hindi_sentence'].apply(lambda x: x.lower())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning - Remove Quotes, Special Characters, Digits, and Extra Spaces"
      ],
      "metadata": {
        "id": "aXecQGZ6Sybj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove single quotes\n",
        "data['english_sentence'] = data['english_sentence'].apply(lambda x: re.sub(\"'\", '', x))\n",
        "data['hindi_sentence'] = data['hindi_sentence'].apply(lambda x: re.sub(\"'\", '', x))\n",
        "\n",
        "# Define punctuation to exclude\n",
        "to_exclude = set(string.punctuation)\n",
        "\n",
        "# Remove special characters\n",
        "data['english_sentence'] = data['english_sentence'].apply(lambda x: ''.join(ch for ch in x if ch not in to_exclude))\n",
        "data['hindi_sentence'] = data['hindi_sentence'].apply(lambda x: ''.join(ch for ch in x if ch not in to_exclude))\n",
        "\n",
        "# Remove digits\n",
        "remove_digits = str.maketrans('', '', digits)\n",
        "data['hindi_sentence'] = data['hindi_sentence'].apply(lambda x: x.translate(remove_digits))\n"
      ],
      "metadata": {
        "id": "HjYJXzWfS5sS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# More Data Cleaning - Removing Specific Hindi Characters and Extra Spaces"
      ],
      "metadata": {
        "id": "nnUNPHFWS99Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove specific Hindi characters\n",
        "data['hindi_sentence'] = data['hindi_sentence'].apply(lambda x: re.sub(\"[२३०८१५७९४६]\", \"\", x))\n",
        "\n",
        "# Remove extra spaces\n",
        "data['english_sentence'] = data['english_sentence'].apply(lambda x: x.strip())\n",
        "data['hindi_sentence'] = data['hindi_sentence'].apply(lambda x: x.strip())\n",
        "data['english_sentence'] = data['english_sentence'].apply(lambda x: re.sub(\" +\", \" \", x))\n",
        "data['hindi_sentence'] = data['hindi_sentence'].apply(lambda x: re.sub(\" +\", \" \", x))\n"
      ],
      "metadata": {
        "id": "2k7wawjxTCqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract Input and Target Sentences, Tokenization"
      ],
      "metadata": {
        "id": "2HJ2lfM3TFQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract input and target sentences\n",
        "input_text = []\n",
        "target_text = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "\n",
        "for eng, hin in data[['english_sentence', 'hindi_sentence']].itertuples(index=False):\n",
        "    target = 'START_' + hin + '_END'  # Add start and end tokens to target\n",
        "    input_text.append(eng)\n",
        "    target_text.append(target)\n",
        "\n",
        "    # Update character sets\n",
        "    for eng_char in eng.split():\n",
        "        if eng_char not in input_characters:\n",
        "            input_characters.add(eng_char)\n",
        "\n",
        "    for hin_char in hin.split():\n",
        "        if hin_char not in target_characters:\n",
        "            target_characters.add(hin_char)\n"
      ],
      "metadata": {
        "id": "TiaTHAeaTMk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Statistics and Token Index Dictionaries"
      ],
      "metadata": {
        "id": "uN5sgxZjTPi6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print dataset statistics\n",
        "print(\"Number of samples:\", len(input_text))\n",
        "print(\"Number of unique input tokens:\", len(input_characters))\n",
        "print(\"Number of unique output tokens:\", len(target_characters))\n",
        "print(\"Max sequence length for inputs:\", max([len(txt) for txt in input_text]))\n",
        "print(\"Max sequence length for outputs:\", max([len(txt) for txt in target_text]))\n",
        "\n",
        "# Create token index dictionaries\n",
        "input_char = sorted(list(input_characters))\n",
        "target_char = sorted(list(target_characters))\n",
        "\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters) + 1  # Add 1 for padding token\n",
        "\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_text])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_text])\n",
        "\n",
        "input_token_index = dict([(word, i + 1) for i, word in enumerate(input_char)])\n",
        "target_token_index = dict([(word, i + 1) for i, word in enumerate(target_char)])\n",
        "\n",
        "reverse_input_char_index = dict((i, word) for word, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, word) for word, i in target_token_index.items())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CS50XdhxTWW1",
        "outputId": "a1fdb3fd-2858-4a1f-9735-e250753c89eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 18416\n",
            "Number of unique input tokens: 9729\n",
            "Number of unique output tokens: 8665\n",
            "Max sequence length for inputs: 30\n",
            "Max sequence length for outputs: 40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save and Load Token Index Dictionaries"
      ],
      "metadata": {
        "id": "lVw34z7HTZCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save token index dictionaries to files using pickle\n",
        "import pickle\n",
        "\n",
        "pickle.dump(input_token_index, open('eng_input_token_index.pickle', 'wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
        "pickle.dump(target_token_index, open('hin_target_token_index.pickle', 'wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
        "pickle.dump(reverse_input_char_index, open('eng_reverse_input_char_index.pickle', 'wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
        "pickle.dump(reverse_target_char_index, open('hin_reverse_target_char_index.pickle', 'wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# Load token index dictionaries from files using pickle\n",
        "with open('eng_input_token_index.pickle', 'rb') as fp:\n",
        "    input_token_index = pickle.load(fp)\n",
        "with open('hin_target_token_index.pickle', 'rb') as fp:\n",
        "    target_token_index = pickle.load(fp)\n",
        "with open('eng_reverse_input_char_index.pickle', 'rb') as fp:\n",
        "    reverse_input_char_index = pickle.load(fp)\n",
        "with open('hin_reverse_target_char_index.pickle', 'rb') as fp:\n",
        "    reverse_target_char_index = pickle.load(fp)"
      ],
      "metadata": {
        "id": "7A2nmpzxTe4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split Data into Train and Test Sets"
      ],
      "metadata": {
        "id": "_FlzLAkDTibB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into train and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = data.english_sentence, data.hindi_sentence\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=2)\n"
      ],
      "metadata": {
        "id": "4-GA3vnTTrfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define a Generator Function for Training Batches"
      ],
      "metadata": {
        "id": "yevcMO-7Ttyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a generator function to generate training batches\n",
        "def generate_batch(X, y, batch_size):\n",
        "    while True:\n",
        "        for j in range(0, len(X), batch_size):\n",
        "            encoder_input_data = np.zeros((batch_size, max_encoder_seq_length), dtype='float32')\n",
        "            decoder_input_data = np.zeros((batch_size, max_decoder_seq_length), dtype='float32')\n",
        "            decoder_target_data = np.zeros((batch_size, max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
        "            for i, (input_text, target_text) in enumerate(zip(X[j:j + batch_size], y[j:j + batch_size])):\n",
        "                for t, word in enumerate(input_text.split()):\n",
        "                    encoder_input_data[i, t] = input_token_index[word]  # encoder input seq\n",
        "                for t, word in enumerate(target_text.split()):\n",
        "                    if t < len(target_text.split()) - 1:\n",
        "                        decoder_input_data[i, t] = target_token_index[word]  # decoder input seq\n",
        "                    if t > 0:\n",
        "                        decoder_target_data[i, t - 1, target_token_index[word]] = 1\n",
        "            yield ([encoder_input_data, decoder_input_data], decoder_target_data)\n"
      ],
      "metadata": {
        "id": "E4RaV50xTybK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Define Model Parameters and Encoder Layers"
      ],
      "metadata": {
        "id": "6DLf-O9hT1BX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model parameters\n",
        "latent_dim = 50\n",
        "\n",
        "num_decoder_tokens = len(target_characters) + 1\n",
        "\n",
        "# Define the encoder\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "enc_emb = Embedding(num_encoder_tokens, latent_dim, mask_zero=True)(encoder_inputs)\n",
        "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
        "encoder_states = [state_h, state_c]\n"
      ],
      "metadata": {
        "id": "MGtjr5hiT5FM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Decoder Layers and Model"
      ],
      "metadata": {
        "id": "kedHsI73T8Vk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the decoder\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "dec_emb_layer = Embedding(num_decoder_tokens, latent_dim, mask_zero=True)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n"
      ],
      "metadata": {
        "id": "1pupMBGhT7K7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compile Model with Custom Learning Rate Optimizer"
      ],
      "metadata": {
        "id": "1pIPSVg1UDod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer with custom learning rate\n",
        "custom_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)  # Set your learning rate here\n",
        "model.compile(optimizer=custom_optimizer, loss='categorical_crossentropy', metrics=['acc'])\n"
      ],
      "metadata": {
        "id": "kXdQ0T6IUC6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Print Model Summary"
      ],
      "metadata": {
        "id": "z-D1t7FLUNAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NI290RM4URdl",
        "outputId": "c959a2b9-240e-4280-c462-1365e76ac075"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, None, 50)             486450    ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)     (None, None, 50)             433300    ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " lstm (LSTM)                 [(None, 50),                 20200     ['embedding[0][0]']           \n",
            "                              (None, 50),                                                         \n",
            "                              (None, 50)]                                                         \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)               [(None, None, 50),           20200     ['embedding_1[0][0]',         \n",
            "                              (None, 50),                            'lstm[0][1]',                \n",
            "                              (None, 50)]                            'lstm[0][2]']                \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, None, 8666)           441966    ['lstm_1[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1402116 (5.35 MB)\n",
            "Trainable params: 1402116 (5.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Training Parameters"
      ],
      "metadata": {
        "id": "KHwVliDPUd--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training parameters (optimized batch_size and epochs)\n",
        "train_samples = len(X_train)\n",
        "val_samples = len(X_test)\n",
        "batch_size = 128  # Adjust batch size as needed\n",
        "epochs = 45  # Adjust the number of epochs as needed\n"
      ],
      "metadata": {
        "id": "VQAz02LgUVbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the Model"
      ],
      "metadata": {
        "id": "6agEgLzZUqRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit_generator(\n",
        "    generator=generate_batch(X_train, y_train, batch_size=batch_size),\n",
        "    steps_per_epoch=train_samples // batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=generate_batch(X_test, y_test, batch_size=batch_size),\n",
        "    validation_steps=val_samples // batch_size\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wtoGrVhdUlEI",
        "outputId": "5db20e2d-c38f-4054-df97-8602689aebb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-c06b0b25cab3>:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  model.fit_generator(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/45\n",
            " 16/129 [==>...........................] - ETA: 3:39 - loss: 9.0055 - acc: 0.0476"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-c06b0b25cab3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.fit_generator(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_samples\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2808\u001b[0m             \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2809\u001b[0m         )\n\u001b[0;32m-> 2810\u001b[0;31m         return self.fit(\n\u001b[0m\u001b[1;32m   2811\u001b[0m             \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2812\u001b[0m             \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'model/embedding/embedding_lookup' defined at (most recent call last):\n    File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n      ColabKernelApp.launch_instance()\n    File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n      lambda f: self._run_callback(functools.partial(callback, future))\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n      ret = callback()\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n      self.ctx_run(self.run)\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n      yielded = self.gen.send(value)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n      yield self.process_one()\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 250, in wrapper\n      runner = Runner(ctx_run, result, future, yielded)\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 748, in __init__\n      self.ctx_run(self.run)\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n      yielded = self.gen.send(value)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n      yield gen.maybe_future(dispatch(*args))\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n      yield gen.maybe_future(handler(stream, idents, msg))\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n      self.do_execute(\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n      return runner(coro)\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-16-d45d87910597>\", line 2, in <cell line: 2>\n      model.fit_generator(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2810, in fit_generator\n      return self.fit(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1742, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1338, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1322, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1303, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1080, in train_step\n      y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 569, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py\", line 512, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py\", line 669, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py\", line 272, in call\n      out = tf.nn.embedding_lookup(self.embeddings, inputs)\nNode: 'model/embedding/embedding_lookup'\nindices[75,0] = 9729 is not in [0, 9729)\n\t [[{{node model/embedding/embedding_lookup}}]] [Op:__inference_train_function_14657]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Save Model Weights"
      ],
      "metadata": {
        "id": "AlAmiq-FUzF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model weights\n",
        "model.save_weights('nmt_eng_hin_translation.h5')\n"
      ],
      "metadata": {
        "id": "GhgsZb8RU88x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Encoder Model"
      ],
      "metadata": {
        "id": "_H9VLcAtU-w1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the encoder model\n",
        "encoder_model = Model(encoder_inputs, encoder_states)"
      ],
      "metadata": {
        "id": "CKVDdmFpVQkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Decoder Model and Decoding Function"
      ],
      "metadata": {
        "id": "s6ngJjfbVVK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the decoder model\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
        "decoder_states2 = [state_h2, state_c2]\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "\n",
        "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs2] + decoder_states2)\n",
        "\n",
        "# Define a function to decode sequences\n",
        "def decode_sequence(input_seq):\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = target_token_index['START_']\n",
        "    decoded_sentence = ''\n",
        "\n",
        "    while True:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "\n",
        "        if sampled_char == '_END' or len(decoded_sentence.split()) > max_decoder_seq_length:\n",
        "            break\n",
        "\n",
        "        decoded_sentence += ' ' + sampled_char\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence.strip()\n"
      ],
      "metadata": {
        "id": "xggKod9TVXn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Validation Data and Decode Sequences"
      ],
      "metadata": {
        "id": "T7gPKcfoVaKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a batch of validation data\n",
        "val_gen = generate_batch(X_test, y_test, batch_size=1)\n",
        "k = -1\n",
        "\n",
        "# Iterate over validation samples and decode\n",
        "k += 2\n",
        "(input_seq, actual_output), _ = next(val_gen)\n",
        "decoded_sentence = decode_sequence(input_seq)\n",
        "\n",
        "# Print results\n",
        "print('Input English sentence:', X_test[k:k+1].values[0])\n",
        "print('Actual Hindi Translation:', y_test[k:k+1].values[0])\n",
        "print('Predicted Hindi Translation:', decoded_sentence)"
      ],
      "metadata": {
        "id": "ug1eESaXVdLQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}